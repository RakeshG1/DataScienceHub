{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right, these concepts are all interconnected! Here's a breakdown to clarify the relationship between derivatives, calculus, and differentiation, along with simple Python examples:\n",
    "\n",
    "**1. Calculus:**\n",
    "\n",
    "- Calculus is a vast branch of mathematics that deals with **change**. It encompasses two main areas:\n",
    "    - **Differential Calculus:** This branch focuses on **rates of change** and **instantaneous changes**. It's where the concept of derivatives comes in.\n",
    "    - **Integral Calculus:** This branch focuses on **accumulation** and finding the **total area or volume** under a curve.\n",
    "\n",
    "**2. Differentiation:**\n",
    "\n",
    "- Differentiation is a fundamental operation in differential calculus. It's a process that allows you to find the **derivative** of a function.\n",
    "- The derivative tells you the **instantaneous rate of change** of a function at a specific point.\n",
    "\n",
    "**3. Derivatives:**\n",
    "\n",
    "- As mentioned above, a derivative is the result of the differentiation process. It represents the **instantaneous rate of change** of a function at a specific input value.\n",
    "\n",
    "**Simple Python Examples:**\n",
    "\n",
    "**Example 1: Position and Velocity (Understanding Rate of Change)**\n",
    "\n",
    "```python\n",
    "```\n",
    "\n",
    "- The `position` function defines how the object's position changes over time.\n",
    "- The `velocity` function calculates the derivative of the `position` function. It represents the object's **instantaneous speed** at a particular time.\n",
    "\n",
    "**Example 2: Cost Function and Marginal Cost (Understanding Profit Change)**\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "- The `cost_function` defines how the total cost changes as the quantity produced increases.\n",
    "- The `marginal_cost` function is the derivative of the `cost_function`. It represents the **additional cost** incurred for producing one more unit.\n",
    "\n",
    "In essence, calculus provides the framework (differential and integral calculus), and differentiation is the specific operation within differential calculus that yields derivatives. Derivatives are the powerful tools that help us understand rates of change and solve various problems in science, engineering, economics, and many other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position at 3 seconds: 9\n",
      "Velocity at 3 seconds: 6\n"
     ]
    }
   ],
   "source": [
    "def position(time):\n",
    "  \"\"\"Simple function representing an object's position over time.\"\"\"\n",
    "  return time**2  # This could represent constant acceleration\n",
    "\n",
    "def velocity(time):\n",
    "  \"\"\"Derivative of the position function, representing instantaneous speed.\"\"\"\n",
    "  return 2 * time  # Rate of change of position (speed)\n",
    "\n",
    "time = 3  # Seconds\n",
    "current_position = position(time)\n",
    "current_velocity = velocity(time)\n",
    "\n",
    "print(\"Position at\", time, \"seconds:\", current_position)\n",
    "print(\"Velocity at\", time, \"seconds:\", current_velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for 100 units produced: 210\n",
      "Marginal cost for each additional unit: 2\n"
     ]
    }
   ],
   "source": [
    "def cost_function(quantity, variable_cost):\n",
    "  \"\"\"Function representing the total cost of producing a certain quantity.\"\"\"\n",
    "  fixed_cost = 10  # Constant cost (e.g., rent)\n",
    "  return fixed_cost + quantity * variable_cost\n",
    "\n",
    "def marginal_cost(variable_cost):\n",
    "  \"\"\"Derivative of the cost function, representing the additional cost per unit.\"\"\"\n",
    "  return variable_cost  # Constant cost per unit\n",
    "\n",
    "quantity_produced = 100\n",
    "# Assuming you have a variable_cost value (e.g., 2)\n",
    "variable_cost = 2\n",
    "total_cost = cost_function(quantity_produced, variable_cost)\n",
    "additional_cost_per_unit = marginal_cost(variable_cost)\n",
    "\n",
    "print(\"Total cost for\", quantity_produced, \"units produced:\", total_cost)\n",
    "print(\"Marginal cost for each additional unit:\", additional_cost_per_unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, a derivative tells you the **instantaneous rate of change** of a function at a specific point. Imagine you're riding a bike: the derivative would be your speed at any given moment.\n",
    "\n",
    "Here's an analogy:\n",
    "\n",
    "* Imagine a hill represented by a function.\n",
    "* The derivative would be the slope of the hill at any point. A steeper slope means the function's value is changing more rapidly at that point.\n",
    "\n",
    "**Python Example:**\n",
    "\n",
    "Let's say we have a function that calculates the area of a square (`f(x) = x^2`), where `x` is the side length. We want to find the derivative of this function to understand how the area changes as the side length increases.\n",
    "\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `area_of_square` defines the function for calculating the area.\n",
    "- `derivative` estimates the derivative using a simplified approach. It calculates the change in the function's output for a very small change in the input (`h`). This gives an approximation of the slope at that point.\n",
    "- We calculate the area of a square with a side length of 5 and then estimate the rate of change of the area at that side length.\n",
    "\n",
    "The result will show that the rate of change (derivative) is 10. This means for every unit increase in side length, the area increases by 10 units squared.\n",
    "\n",
    "**Remember:** This is a simplified example. In practice, calculating derivatives might involve more complex mathematical formulas. However, it captures the essence of what derivatives tell us: the rate of change of a function at any specific point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area of a square with side length 5 : 25\n",
      "Rate of change of area with respect to side length: 10.000099999984968\n"
     ]
    }
   ],
   "source": [
    "def area_of_square(side_length):\n",
    "  \"\"\"Calculates the area of a square.\"\"\"\n",
    "  return side_length * side_length\n",
    "\n",
    "def derivative(function, input_value):\n",
    "  \"\"\"Estimates the derivative of a function at a given input.\"\"\"\n",
    "  # This is a simplified approach for illustration purposes.\n",
    "  # In practice, more sophisticated methods are used.\n",
    "  h = 0.0001  # A very small change in input (h)\n",
    "  output_with_change = function(input_value + h)\n",
    "  output_without_change = function(input_value)\n",
    "  estimated_derivative = (output_with_change - output_without_change) / h\n",
    "  return estimated_derivative\n",
    "\n",
    "side_length = 5\n",
    "area = area_of_square(side_length)\n",
    "rate_of_change = derivative(area_of_square, side_length)\n",
    "\n",
    "print(\"Area of a square with side length\", side_length, \":\", area)\n",
    "print(\"Rate of change of area with respect to side length:\", rate_of_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd be glad to explain how derivatives are instrumental in data science modeling, along with a clear Python example:\n",
    "\n",
    "**Derivatives in Data Science Modeling**\n",
    "\n",
    "Derivatives, a fundamental concept in calculus, measure the instantaneous rate of change of a function at a specific point. In data science, this translates to understanding how changes in input features (independent variables) affect the output or prediction (dependent variable) of a model. This knowledge is crucial for:\n",
    "\n",
    "- **Gradient Descent Optimization:** This widely used algorithm underpins many machine learning models like linear regression, neural networks, and decision trees. It iteratively adjusts the model's internal parameters (weights and biases) in a direction that minimizes the error between the model's predictions and the actual data points. Derivatives tell us how much the error function changes with respect to each parameter, guiding the algorithm to take steps that progressively reduce the error.\n",
    "- **Feature Selection:** Analyzing the derivatives of a model's output with respect to each input feature can reveal which features have the most significant impact on the predictions. This can help identify irrelevant or redundant features, leading to a more concise and interpretable model.\n",
    "\n",
    "**Python Example: Linear Regression with Gradient Descent**\n",
    "\n",
    "Let's consider a simple linear regression model represented by the equation:\n",
    "\n",
    "```\n",
    "y = mx + b\n",
    "```\n",
    "\n",
    "where:\n",
    "- `y` is the predicted output\n",
    "- `x` is the input feature\n",
    "- `m` is the slope (weight)\n",
    "- `b` is the y-intercept (bias)\n",
    "\n",
    "We want to fit this model to a dataset and minimize the error between the predicted `y` values and the actual data points. Here's how derivatives come into play:\n",
    "\n",
    "In this example, the `gradient_descent` function calculates the derivatives of the mean squared error function with respect to the slope (`m`) and bias (`b`). These derivatives guide the update of the parameters in each iteration, minimizing the error and improving the model's fit to the data.\n",
    "\n",
    "By understanding how derivatives help us optimize model parameters and gain insights into feature importance, you can leverage this knowledge to build more effective and interpretable data science models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet calculates the derivative of the mean squared error (MSE) with respect to the slope (m) in linear regression. Here's a breakdown of the explanation and the concept of anomaly:\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "1. **Mean Squared Error (MSE):** It's a common metric used to evaluate the performance of regression models. It measures the average squared difference between the predicted values (`y_predicted`) and the actual target values (`y`) in your data. Lower MSE indicates a better fit of the model to the data.\n",
    "\n",
    "2. **Derivative:** The derivative of a function measures its instantaneous rate of change at a specific point. In this context, we're calculating the derivative of the MSE function with respect to the slope (m) of the linear regression line.\n",
    "\n",
    "3. **Gradient Descent:** This is an optimization algorithm widely used in machine learning to train models. It iteratively adjusts the model's parameters (like slope and intercept) to minimize the MSE. The derivative we're calculating tells us how much the MSE changes for a small change in the slope, which guides the gradient descent algorithm to update the slope in the direction that reduces the error the most.\n",
    "\n",
    "**Breakdown of the Code:**\n",
    "\n",
    "   - `dm = -2 * np.mean(x * (y - y_predicted))`:\n",
    "     - `dm`: This variable stores the calculated derivative (change in MSE) with respect to the slope (m).\n",
    "     - `-2`: This factor accounts for the constant multiplier in the MSE formula.\n",
    "     - `np.mean`: This NumPy function calculates the average of the elements within the parentheses.\n",
    "     - `x * (y - y_predicted)`: This part calculates the difference between actual values (`y`) and predicted values (`y_predicted`) for each data point, multiplied by the corresponding input feature (`x`).\n",
    "\n",
    "**Anomaly**\n",
    "\n",
    "The concept of anomaly in this context doesn't directly relate to the code itself. However, anomalies in your data can affect the calculation of the derivative and the performance of the model:\n",
    "\n",
    "- **Outliers:** If your data has outliers (extreme values that deviate significantly from the majority), they can disproportionately influence the mean squared error calculation and its derivative. This might lead the gradient descent algorithm to take steps that are not optimal for the overall trend of the data.\n",
    "- **Non-linear Relationships:** The linear regression model assumes a linear relationship between the features (`x`) and the target variable (`y`). If your data exhibits a non-linear relationship, the MSE and its derivative might not accurately reflect the model's errors, leading to suboptimal performance.\n",
    "\n",
    "It's essential to consider these potential issues when interpreting the derivative and using it for model optimization. Techniques like outlier detection and handling, or choosing appropriate models for non-linear data can help mitigate the impact of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Slope (m): 1.4260044934291738\n",
      "Optimized Intercept (b): 0.6295313675863407\n",
      "Predicted Values: [2.05553586 3.48154035 4.90754485 6.33354934]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_predicted):\n",
    "  \"\"\"Calculates the mean squared error between true and predicted values.\"\"\"\n",
    "  return np.mean((y_true - y_predicted) ** 2)\n",
    "\n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "  \"\"\"Implements gradient descent to optimize model parameters.\"\"\"\n",
    "  m = 0  # Initial slope\n",
    "  b = 0  # Initial bias\n",
    "\n",
    "  for _ in range(num_iterations):\n",
    "    # Calculate predictions with current parameters\n",
    "    y_predicted = m * x + b\n",
    "\n",
    "    # Calculate the derivative of the mean squared error w.r.t. slope (m)\n",
    "    dm = -2 * np.mean(x * (y - y_predicted))\n",
    "\n",
    "    # Calculate the derivative of the mean squared error w.r.t. bias (b)\n",
    "    db = -2 * np.mean(y - y_predicted)\n",
    "\n",
    "    # Update parameters using the derivatives and learning rate\n",
    "    m -= learning_rate * dm\n",
    "    b -= learning_rate * db\n",
    "\n",
    "  return m, b\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([2, 4, 5, 6])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# Train the model using gradient descent\n",
    "slope, intercept = gradient_descent(x, y, learning_rate, num_iterations)\n",
    "\n",
    "# Make predictions with the optimized parameters\n",
    "predicted_y = slope * x + intercept\n",
    "\n",
    "print(\"Optimized Slope (m):\", slope)\n",
    "print(\"Optimized Intercept (b):\", intercept)\n",
    "print(\"Predicted Values:\", predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://python.tutorialink.com/numpy-second-derivative-of-a-ndimensional-array/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data --> [0.750631487521711, 0.6723610238402501, 0.7765331360539932, 0.671602960702756, 0.2284641101581274]\n",
      "first_derivative --> [-0.07827046368146096, 0.012950824266141081, -0.0003790315687470236, -0.2740345129479329, -0.44313885054462865]\n",
      "----------------------------------------\n",
      "data --> [10, 30, 40, 50, 5, 12]\n",
      "first_derivative --> [20.0, 15.0, 10.0, -17.5, -19.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data = np.random.rand(30,50,40,20)\n",
    "data = np.random.rand(5).tolist() # tolist(): converting array to list\n",
    "# print(\"type(data) -->\", type(data))\n",
    "print(\"data -->\", data)\n",
    "first_derivative = np.gradient(data).tolist()\n",
    "print(\"first_derivative -->\", first_derivative)\n",
    "print(\"--\"*20)\n",
    "\n",
    "data = [10, 30, 40 , 50, 5, 12]\n",
    "print(\"data -->\", data)\n",
    "first_derivative = np.gradient(data).tolist()\n",
    "print(\"first_derivative -->\", first_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.py4u.net/discuss/146353\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y -->  [10, 30, 40, 50, 5, 12]\n",
      "First_derivative : dy --> [ 20.  10.  10. -45.   7.]\n",
      "Second_derivative : dy2 --> [-10.   0. -55.  52.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import diff\n",
    "\n",
    "dx = 1\n",
    "x_list = [1,1,1,1,1,1]\n",
    "y = [10, 30, 40 , 50, 5, 12]\n",
    "print(\"y --> \", y)\n",
    "dy = diff(y)/dx\n",
    "print(\"First_derivative : dy -->\", dy)\n",
    "dy2 = diff(dy)/dx\n",
    "print(\"Second_derivative : dy2 -->\", dy2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://stackoverflow.com/questions/45063260/finding-the-point-of-a-slope-change-as-a-free-parameter-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code effectively demonstrates how to find the point of a slope change in Python using derivatives. Here's a breakdown of the steps:\n",
    "\n",
    "1. **Data:**\n",
    "   - It defines a list `y` containing sample data points.\n",
    "\n",
    "2. **First Derivative (dy):**\n",
    "   - This list stores the difference between consecutive elements in `y`. This essentially calculates the slope between each pair of points.\n",
    "   - The loop iterates through `y` (excluding the last element) and calculates `y[i+1] - y[i]`, which represents the change in `y` between the current and next index.\n",
    "\n",
    "3. **Second Derivative (dpy):**\n",
    "   - This list stores the difference between consecutive elements in `dy`. This essentially calculates the change in slope between each pair of points in the first derivative.\n",
    "   - It follows the same logic as the first derivative calculation, but applied to the `dy` list.\n",
    "\n",
    "4. **Finding the Change Point:**\n",
    "   - It uses `dpy.index(1)` to find the index where the second derivative is 1. We're looking for a non-zero value in `dpy` because a change in slope occurs when the first derivative (slope) itself changes.\n",
    "\n",
    "5. **Value at the Change Point:**\n",
    "   - `y[change]` retrieves the value from the original data list (`y`) at the index identified by the change point (`change`). This represents the data point where the slope changes.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This code effectively utilizes the concept of derivatives to identify the point of slope change in a data series. By calculating the first derivative (slope) and then its derivative (change in slope), it pinpoints the transition between different slopes.\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- This approach assumes a smooth and continuous function representing the data. In cases of abrupt changes or noise, it may require additional handling.\n",
    "- For more complex data, consider using libraries like `scipy` for more robust derivative calculations, especially if dealing with noisy data or discontinuities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Values -->  [1, 2, 3, 4, 5, 6, 8, 10, 12, 14]\n",
      "First derivative[dy] -->  [1, 1, 1, 1, 1, 2, 2, 2, 2]\n",
      "Second derivative[dpy] -->  [0, 0, 0, 0, 1, 0, 0, 0]\n",
      "change : dpy.index(1) -->  4\n",
      "y[change] -->  5\n"
     ]
    }
   ],
   "source": [
    "y = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14]\n",
    "print(\"Data Values --> \", y)\n",
    "dy=[y[i+1]-y[i] for i in range(len(y)-1)]\n",
    "print(\"First derivative[dy] --> \", dy)\n",
    "\n",
    "# and then find the point where it change (second derivative):\n",
    "dpy=[dy[i+1]-dy[i] for i in range(len(dy)-1)]\n",
    "print(\"Second derivative[dpy] --> \", dpy)\n",
    "\n",
    "# if you want the index of this point :\n",
    "print(\"change : dpy.index(1) --> \", dpy.index(1))\n",
    "\n",
    "# that can give you the value of the last point before change of slope :\n",
    "change=dpy.index(1)\n",
    "print(\"y[change] --> \", y[change])\n",
    "\n",
    "\n",
    "# In your y = [1, 2, 3, 4, 5, 6, 8, 10, 12, 14]\n",
    "# the change happen at the index [4] (list indexing start to 0) and the value of y at this point is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://newbedev.com/finding-local-maxima-minima-with-numpy-in-a-1d-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37372019 0.14567833 0.12308628 0.71103505 0.15529211 0.70066729\n",
      " 0.52214991 0.91060156 0.84481548 0.36651489 0.66922714 0.3016033 ]\n",
      "(array([ 3,  5,  7, 10], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "x = np.random.random(12)\n",
    "# x = df_temp.dbl_v.values # Array\n",
    "print(x)\n",
    "\n",
    "# for local maxima\n",
    "print(argrelextrema(x, np.greater))\n",
    "\n",
    "# for local minima\n",
    "# print(argrelextrema(x, np.less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
